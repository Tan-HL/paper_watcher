#!/usr/bin/env python3
"""
è®ºæ–‡ç¬”è®°è‡ªåŠ¨åŒ–å·¥å…·
åŠŸèƒ½ï¼š
1. ç›‘æ§Markdownæ–‡ä»¶å˜åŒ–
2. æ£€æµ‹æ–°å¢çš„è®ºæ–‡é“¾æ¥ï¼ˆæ”¯æŒarXivã€DOIï¼‰
3. è‡ªåŠ¨ä¸‹è½½PDF
4. è·å–è®ºæ–‡å…ƒæ•°æ®ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€å¼•ç”¨æ•°ï¼‰
5. ç”Ÿæˆæ ¼å¼åŒ–çš„å¼•ç”¨ä¿¡æ¯
"""

import os
import re
import time
import json
import hashlib
import requests
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, List, Tuple
from dataclasses import dataclass
from urllib.parse import urlparse, unquote

try:
    from watchdog.observers import Observer
    from watchdog.events import FileSystemEventHandler, FileModifiedEvent
except ImportError:
    print("è¯·å®‰è£… watchdog: pip install watchdog")
    exit(1)

# ==================== é…ç½® ====================
CONFIG = {
    "watch_dir": "./papers",           # ç›‘æ§çš„ç›®å½•
    "pdf_dir": "./papers/pdfs",        # PDFä¿å­˜ç›®å½•
    "state_file": ".paper_watcher_state.json",  # çŠ¶æ€æ–‡ä»¶
    "check_interval": 2,               # æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
    # ä»£ç†é…ç½®ï¼ˆClashé»˜è®¤ç«¯å£ï¼‰
    "proxy": {
        "http": "http://127.0.0.1:7897",
        "https": "http://127.0.0.1:7897"
    },
    "use_proxy": True,                 # æ˜¯å¦ä½¿ç”¨ä»£ç†
}

# ==================== æ•°æ®ç±» ====================

def get_proxies():
    """è·å–ä»£ç†é…ç½®"""
    if CONFIG.get("use_proxy", False):
        return CONFIG.get("proxy", {})
    return None

@dataclass
class PaperInfo:
    title: str
    authors: List[str]
    venue: str  # æœŸåˆŠ/ä¼šè®®
    year: str
    arxiv_id: Optional[str] = None
    doi: Optional[str] = None
    pdf_url: Optional[str] = None
    citations: Optional[int] = None
    
    def format_authors(self, max_authors: int = 3) -> str:
        """æ ¼å¼åŒ–ä½œè€…åˆ—è¡¨"""
        if len(self.authors) <= max_authors:
            return ", ".join(self.authors)
        return f"{', '.join(self.authors[:max_authors])} et al."
    
    def to_markdown(self, local_pdf_path: Optional[str] = None) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼çš„å¼•ç”¨
        æ ¼å¼: æ ‡é¢˜.ä½œè€….æœŸåˆŠ/ä¼šè®®,å¹´ä»½ ([PDF](é“¾æ¥)) ([arXiv](é“¾æ¥)) (Citations: æ•°é‡)
        """
        # åŸºæœ¬ä¿¡æ¯
        result = f"**{self.title}**. {self.format_authors()}. {self.venue}, {self.year}"
        
        # é“¾æ¥éƒ¨åˆ†
        links = []
        if local_pdf_path:
            links.append(f"[PDF]({local_pdf_path})")
        if self.arxiv_id:
            links.append(f"[arXiv](https://arxiv.org/abs/{self.arxiv_id})")
        if self.doi:
            links.append(f"[DOI](https://doi.org/{self.doi})")
        
        # å¼•ç”¨æ•°
        citation_str = f"Citations: {self.citations}" if self.citations is not None else "Citations: N/A"
        
        # ç»„åˆï¼šæ¯ä¸ªé“¾æ¥ç”¨æ‹¬å·åŒ…è£¹
        if links:
            result += " " + " ".join(f"({link})" for link in links)
        result += f" ({citation_str})"
        
        return result


# ==================== URLè§£æå™¨ ====================
class URLParser:
    """è§£æè®ºæ–‡URLï¼Œæå–ID"""
    
    # arXiv URLæ¨¡å¼
    ARXIV_PATTERNS = [
        r'arxiv\.org/abs/(\d{4}\.\d{4,5}(?:v\d+)?)',
        r'arxiv\.org/pdf/(\d{4}\.\d{4,5}(?:v\d+)?)',
        r'arxiv\.org/abs/([a-z-]+/\d{7})',
        r'arxiv\.org/pdf/([a-z-]+/\d{7})',
    ]
    
    # DOI URLæ¨¡å¼
    DOI_PATTERNS = [
        r'doi\.org/(10\.\d{4,}/[^\s\)]+)',
        r'doi:\s*(10\.\d{4,}/[^\s\)]+)',
    ]
    
    @classmethod
    def extract_arxiv_id(cls, url: str) -> Optional[str]:
        """ä»URLæå–arXiv ID"""
        for pattern in cls.ARXIV_PATTERNS:
            match = re.search(pattern, url, re.IGNORECASE)
            if match:
                arxiv_id = match.group(1)
                # å»é™¤ç‰ˆæœ¬å·ç”¨äºæ¯”è¾ƒ
                return arxiv_id.split('v')[0] if 'v' in arxiv_id else arxiv_id
        return None
    
    @classmethod
    def extract_doi(cls, url: str) -> Optional[str]:
        """ä»URLæå–DOI"""
        for pattern in cls.DOI_PATTERNS:
            match = re.search(pattern, url, re.IGNORECASE)
            if match:
                return match.group(1).rstrip('.')
        return None
    
    @classmethod
    def find_urls_in_text(cls, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰URL"""
        url_pattern = r'https?://[^\s\)\]<>\"\']+|doi:\s*10\.\d{4,}/[^\s\)\]<>\"\'"]+'
        urls = re.findall(url_pattern, text)
        return [url.rstrip('.,;:') for url in urls]


# ==================== APIå®¢æˆ·ç«¯ ====================
class ArxivAPI:
    """arXiv APIå®¢æˆ·ç«¯"""
    
    BASE_URL = "https://export.arxiv.org/api/query"
    
    @classmethod
    def get_paper_info(cls, arxiv_id: str) -> Optional[PaperInfo]:
        """é€šè¿‡arXiv IDè·å–è®ºæ–‡ä¿¡æ¯"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            params = {"id_list": arxiv_id}
            response = requests.get(cls.BASE_URL, params=params, headers=headers, 
                                    proxies=get_proxies(), timeout=30)
            response.raise_for_status()
            
            # è§£æXMLå“åº”
            import xml.etree.ElementTree as ET
            root = ET.fromstring(response.text)
            
            ns = {'atom': 'http://www.w3.org/2005/Atom'}
            entry = root.find('atom:entry', ns)
            
            if entry is None:
                return None
            
            title = entry.find('atom:title', ns)
            title_text = title.text.strip().replace('\n', ' ') if title is not None else "Unknown"
            
            authors = []
            for author in entry.findall('atom:author', ns):
                name = author.find('atom:name', ns)
                if name is not None:
                    authors.append(name.text)
            
            published = entry.find('atom:published', ns)
            year = published.text[:4] if published is not None else "Unknown"
            
            # è·å–åˆ†ç±»ä½œä¸ºvenue
            categories = entry.findall('atom:category', ns)
            primary_category = categories[0].get('term') if categories else "arXiv"
            
            # PDFé“¾æ¥
            pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
            
            return PaperInfo(
                title=title_text,
                authors=authors,
                venue=f"arXiv:{primary_category}",
                year=year,
                arxiv_id=arxiv_id,
                pdf_url=pdf_url
            )
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 403:
                print(f"  [è­¦å‘Š] arXiv APIè®¿é—®å—é™ï¼Œå°è¯•ä½¿ç”¨Semantic Scholar...")
                return cls._fallback_semantic_scholar(arxiv_id)
            print(f"  [é”™è¯¯] è·å–arXivä¿¡æ¯å¤±è´¥: {e}")
            return None
        except Exception as e:
            print(f"  [é”™è¯¯] è·å–arXivä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    @classmethod
    def _fallback_semantic_scholar(cls, arxiv_id: str) -> Optional[PaperInfo]:
        """ä½¿ç”¨Semantic Scholarä½œä¸ºå¤‡ç”¨"""
        return SemanticScholarAPI.get_full_paper_info(arxiv_id)


class SemanticScholarAPI:
    """Semantic Scholar APIå®¢æˆ·ç«¯ - ç”¨äºè·å–è®ºæ–‡ä¿¡æ¯å’Œå¼•ç”¨æ•°"""
    
    BASE_URL = "https://api.semanticscholar.org/graph/v1/paper"
    
    @classmethod
    def _request_with_retry(cls, url: str, params: dict, max_retries: int = 3) -> Optional[dict]:
        """å¸¦é‡è¯•æœºåˆ¶çš„è¯·æ±‚"""
        headers = {'User-Agent': 'Mozilla/5.0'}
        
        for attempt in range(max_retries):
            try:
                response = requests.get(url, params=params, headers=headers,
                                        proxies=get_proxies(), timeout=30)
                
                if response.status_code == 200:
                    return response.json()
                elif response.status_code == 429:
                    # è¢«é™é€Ÿï¼Œç­‰å¾…åé‡è¯•
                    wait_time = (attempt + 1) * 3  # 3ç§’, 6ç§’, 9ç§’
                    print(f"  [è­¦å‘Š] APIé™é€Ÿï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"  [è­¦å‘Š] APIè¿”å›: {response.status_code}")
                    return None
            except Exception as e:
                print(f"  [è­¦å‘Š] è¯·æ±‚å¤±è´¥: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2)
                    continue
        return None
    
    @classmethod
    def get_citations(cls, arxiv_id: str = None, doi: str = None) -> Optional[int]:
        """è·å–è®ºæ–‡å¼•ç”¨æ•°"""
        if arxiv_id:
            paper_id = f"arXiv:{arxiv_id}"
        elif doi:
            paper_id = doi
        else:
            return None
        
        url = f"{cls.BASE_URL}/{paper_id}"
        params = {"fields": "citationCount"}
        
        data = cls._request_with_retry(url, params)
        if data:
            return data.get('citationCount')
        return None
    
    @classmethod
    def get_full_paper_info(cls, arxiv_id: str) -> Optional[PaperInfo]:
        """é€šè¿‡arXiv IDè·å–å®Œæ•´è®ºæ–‡ä¿¡æ¯"""
        url = f"{cls.BASE_URL}/arXiv:{arxiv_id}"
        params = {"fields": "title,authors,year,venue,citationCount,externalIds,publicationVenue"}
        
        data = cls._request_with_retry(url, params)
        if not data:
            return None
        
        authors = [a.get('name', 'Unknown') for a in data.get('authors', [])]
        
        # è·å–venueä¿¡æ¯
        venue = data.get('venue') or ''
        pub_venue = data.get('publicationVenue')
        if pub_venue and pub_venue.get('name'):
            venue = pub_venue.get('name')
        if not venue:
            venue = 'arXiv'
        
        return PaperInfo(
            title=data.get('title', 'Unknown'),
            authors=authors,
            venue=venue,
            year=str(data.get('year', 'Unknown')),
            arxiv_id=arxiv_id,
            pdf_url=f"https://arxiv.org/pdf/{arxiv_id}.pdf",
            citations=data.get('citationCount')
        )


# ==================== PDFä¸‹è½½å™¨ ====================
class PDFDownloader:
    """PDFä¸‹è½½å™¨"""
    
    @classmethod
    def download(cls, url: str, save_path: str) -> bool:
        """ä¸‹è½½PDFæ–‡ä»¶"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, proxies=get_proxies(), 
                                    timeout=60, stream=True)
            response.raise_for_status()
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            return True
        except Exception as e:
            print(f"  [é”™è¯¯] ä¸‹è½½PDFå¤±è´¥: {e}")
            return False
    
    @classmethod
    def generate_filename(cls, paper_info: PaperInfo) -> str:
        """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
        # æ¸…ç†æ ‡é¢˜ï¼Œç§»é™¤ä¸å®‰å…¨å­—ç¬¦
        title = re.sub(r'[<>:"/\\|?*]', '', paper_info.title)
        title = title[:80]  # é™åˆ¶é•¿åº¦
        first_author = paper_info.authors[0].split()[-1] if paper_info.authors else "Unknown"
        return f"{first_author}_{paper_info.year}_{title}.pdf"


# ==================== çŠ¶æ€ç®¡ç† ====================
class StateManager:
    """ç®¡ç†å·²å¤„ç†çš„URLçŠ¶æ€"""
    
    def __init__(self, state_file: str):
        self.state_file = state_file
        self.state = self._load_state()
    
    def _load_state(self) -> Dict:
        """åŠ è½½çŠ¶æ€æ–‡ä»¶"""
        if os.path.exists(self.state_file):
            try:
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        return {"processed_urls": {}, "file_hashes": {}}
    
    def save_state(self):
        """ä¿å­˜çŠ¶æ€"""
        with open(self.state_file, 'w', encoding='utf-8') as f:
            json.dump(self.state, f, indent=2, ensure_ascii=False)
    
    def is_url_processed(self, url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦å·²å¤„ç†"""
        return url in self.state["processed_urls"]
    
    def mark_url_processed(self, url: str, paper_info: dict):
        """æ ‡è®°URLä¸ºå·²å¤„ç†"""
        self.state["processed_urls"][url] = {
            "processed_at": datetime.now().isoformat(),
            "info": paper_info
        }
        self.save_state()
    
    def get_file_hash(self, filepath: str) -> str:
        """è·å–æ–‡ä»¶å†…å®¹hash"""
        with open(filepath, 'r', encoding='utf-8') as f:
            return hashlib.md5(f.read().encode()).hexdigest()
    
    def has_file_changed(self, filepath: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰å˜åŒ–"""
        current_hash = self.get_file_hash(filepath)
        old_hash = self.state["file_hashes"].get(filepath)
        return current_hash != old_hash
    
    def update_file_hash(self, filepath: str):
        """æ›´æ–°æ–‡ä»¶hash"""
        self.state["file_hashes"][filepath] = self.get_file_hash(filepath)
        self.save_state()


# ==================== Markdownå¤„ç†å™¨ ====================
class MarkdownProcessor:
    """å¤„ç†Markdownæ–‡ä»¶"""
    
    def __init__(self, state_manager: StateManager, pdf_dir: str):
        self.state = state_manager
        self.pdf_dir = pdf_dir
    
    def process_file(self, filepath: str) -> List[Tuple[str, PaperInfo, str]]:
        """
        å¤„ç†Markdownæ–‡ä»¶ï¼Œè¿”å›æ–°å¤„ç†çš„è®ºæ–‡åˆ—è¡¨
        Returns: [(åŸå§‹URL, PaperInfo, æœ¬åœ°PDFè·¯å¾„), ...]
        """
        results = []
        
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æå–æ‰€æœ‰URL
        urls = URLParser.find_urls_in_text(content)
        
        for url in urls:
            if self.state.is_url_processed(url):
                continue
            
            print(f"\n  å‘ç°æ–°é“¾æ¥: {url}")
            
            # å°è¯•è§£æarXiv ID
            arxiv_id = URLParser.extract_arxiv_id(url)
            if arxiv_id:
                paper_info = self._process_arxiv(arxiv_id)
                if paper_info:
                    # ä¸‹è½½PDF
                    pdf_filename = PDFDownloader.generate_filename(paper_info)
                    pdf_path = os.path.join(self.pdf_dir, pdf_filename)
                    
                    if PDFDownloader.download(paper_info.pdf_url, pdf_path):
                        print(f"  âœ“ PDFå·²ä¸‹è½½: {pdf_filename}")
                        relative_pdf_path = os.path.relpath(pdf_path, os.path.dirname(filepath))
                    else:
                        relative_pdf_path = None
                    
                    results.append((url, paper_info, relative_pdf_path))
                    
                    # æ ‡è®°ä¸ºå·²å¤„ç†
                    self.state.mark_url_processed(url, {
                        "title": paper_info.title,
                        "arxiv_id": arxiv_id
                    })
                    
                    # å¤„ç†å¤šä¸ªé“¾æ¥æ—¶æ·»åŠ å»¶è¿Ÿï¼Œé¿å…APIé™é€Ÿ
                    time.sleep(2)
                continue
            
            # å°è¯•è§£æDOI (åŸºç¡€æ”¯æŒ)
            doi = URLParser.extract_doi(url)
            if doi:
                print(f"  [ä¿¡æ¯] æ£€æµ‹åˆ°DOI: {doi}ï¼Œæš‚æ—¶ä»…æ”¯æŒarXivé“¾æ¥çš„å®Œæ•´å¤„ç†")
                self.state.mark_url_processed(url, {"doi": doi})
        
        return results
    
    def _process_arxiv(self, arxiv_id: str) -> Optional[PaperInfo]:
        """å¤„ç†arXivè®ºæ–‡ - ä¼˜å…ˆä½¿ç”¨Semantic Scholar API"""
        print(f"  æ­£åœ¨è·å–è®ºæ–‡ä¿¡æ¯: {arxiv_id}")
        
        # ä¼˜å…ˆä½¿ç”¨Semantic Scholarï¼ˆæ›´ç¨³å®šï¼Œä¸”åŒ…å«å¼•ç”¨æ•°ï¼‰
        paper_info = SemanticScholarAPI.get_full_paper_info(arxiv_id)
        
        # å¦‚æœSemantic Scholarå¤±è´¥ï¼Œå°è¯•arXiv API
        if not paper_info:
            print(f"  [ä¿¡æ¯] å°è¯•ä½¿ç”¨arXiv API...")
            paper_info = ArxivAPI.get_paper_info(arxiv_id)
        
        if not paper_info:
            print(f"  [é”™è¯¯] æ— æ³•è·å–è®ºæ–‡ä¿¡æ¯")
            return None
        
        print(f"  âœ“ æ ‡é¢˜: {paper_info.title[:60]}...")
        print(f"  âœ“ ä½œè€…: {paper_info.format_authors()}")
        
        # å¦‚æœå¼•ç”¨æ•°è¿˜æ²¡æœ‰ï¼Œå•ç‹¬è·å–ï¼ˆåŠ å»¶è¿Ÿé¿å…é™é€Ÿï¼‰
        if paper_info.citations is None:
            print(f"  æ­£åœ¨è·å–å¼•ç”¨æ•°...")
            time.sleep(1)  # é¿å…è¿ç»­è¯·æ±‚è¢«é™é€Ÿ
            citations = SemanticScholarAPI.get_citations(arxiv_id=arxiv_id)
            if citations is not None:
                paper_info.citations = citations
        
        if paper_info.citations is not None:
            print(f"  âœ“ å¼•ç”¨æ•°: {paper_info.citations}")
        else:
            print(f"  [è­¦å‘Š] æ— æ³•è·å–å¼•ç”¨æ•°")
        
        return paper_info
    
    def update_file_with_formatted_refs(self, filepath: str, 
                                        results: List[Tuple[str, PaperInfo, str]]):
        """æ›´æ–°æ–‡ä»¶ï¼Œåœ¨åŸå§‹URLåé¢è¿½åŠ æ ¼å¼åŒ–çš„å¼•ç”¨ä¿¡æ¯"""
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        for url, paper_info, pdf_path in results:
            formatted = paper_info.to_markdown(pdf_path)
            
            # åœ¨URLåé¢è¿½åŠ æ ¼å¼åŒ–ä¿¡æ¯
            # ä½¿ç”¨å­—ç¬¦ä¸²æ›¿æ¢è€Œéæ­£åˆ™ï¼Œé¿å…ç‰¹æ®Šå­—ç¬¦é—®é¢˜
            # æŸ¥æ‰¾ç‹¬ç«‹çš„URLè¡Œï¼ˆURLå•ç‹¬ä¸€è¡Œï¼‰
            lines = content.split('\n')
            new_lines = []
            for line in lines:
                new_lines.append(line)
                # å¦‚æœè¿™ä¸€è¡Œå°±æ˜¯URLï¼ˆå»é™¤é¦–å°¾ç©ºç™½ååŒ¹é…ï¼‰
                if line.strip() == url:
                    new_lines.append('')  # ç©ºè¡Œ
                    new_lines.append(formatted)
            content = '\n'.join(new_lines)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"  âœ“ å·²æ›´æ–°æ–‡ä»¶: {filepath}")


# ==================== æ–‡ä»¶ç›‘æ§ ====================
class PaperWatcherHandler(FileSystemEventHandler):
    """æ–‡ä»¶å˜åŒ–å¤„ç†å™¨"""
    
    def __init__(self, processor: MarkdownProcessor, state: StateManager):
        self.processor = processor
        self.state = state
        self.pending_files = set()
        self.last_event_time = {}
    
    def on_modified(self, event):
        if event.is_directory:
            return
        
        if not event.src_path.endswith('.md'):
            return
        
        # é˜²æŠ–ï¼šåŒä¸€æ–‡ä»¶2ç§’å†…åªå¤„ç†ä¸€æ¬¡
        current_time = time.time()
        last_time = self.last_event_time.get(event.src_path, 0)
        if current_time - last_time < 2:
            return
        
        self.last_event_time[event.src_path] = current_time
        self.pending_files.add(event.src_path)
    
    def process_pending(self):
        """å¤„ç†å¾…å¤„ç†çš„æ–‡ä»¶"""
        for filepath in list(self.pending_files):
            if not os.path.exists(filepath):
                self.pending_files.discard(filepath)
                continue
            
            print(f"\n{'='*50}")
            print(f"æ£€æµ‹åˆ°æ–‡ä»¶å˜åŒ–: {filepath}")
            
            try:
                results = self.processor.process_file(filepath)
                
                if results:
                    print(f"\nå¤„ç†äº† {len(results)} ç¯‡æ–°è®ºæ–‡:")
                    for url, paper_info, pdf_path in results:
                        print(f"\n  ğŸ“„ {paper_info.title[:50]}...")
                        print(f"     {paper_info.to_markdown(pdf_path)}")
                    
                    # è¯¢é—®æ˜¯å¦æ›´æ–°æ–‡ä»¶
                    self.processor.update_file_with_formatted_refs(filepath, results)
                else:
                    print("  æ²¡æœ‰å‘ç°æ–°çš„è®ºæ–‡é“¾æ¥")
                
                self.state.update_file_hash(filepath)
            except Exception as e:
                print(f"  [é”™è¯¯] å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
            
            self.pending_files.discard(filepath)


# ==================== ä¸»ç¨‹åº ====================
def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='è®ºæ–‡ç¬”è®°è‡ªåŠ¨åŒ–å·¥å…·')
    parser.add_argument('--watch', '-w', type=str, default='./papers',
                        help='ç›‘æ§çš„ç›®å½• (é»˜è®¤: ./papers)')
    parser.add_argument('--pdf-dir', '-p', type=str, default=None,
                        help='PDFä¿å­˜ç›®å½• (é»˜è®¤: ç›‘æ§ç›®å½•/pdfs)')
    parser.add_argument('--once', '-o', action='store_true',
                        help='åªæ‰«æä¸€æ¬¡ï¼Œä¸æŒç»­ç›‘æ§')
    parser.add_argument('--proxy', type=str, default=None,
                        help='ä»£ç†åœ°å€ (ä¾‹å¦‚: http://127.0.0.1:7897)')
    parser.add_argument('--no-proxy', action='store_true',
                        help='ç¦ç”¨ä»£ç†')
    
    args = parser.parse_args()
    
    # å¤„ç†ä»£ç†é…ç½®
    if args.no_proxy:
        CONFIG["use_proxy"] = False
    elif args.proxy:
        CONFIG["use_proxy"] = True
        CONFIG["proxy"] = {
            "http": args.proxy,
            "https": args.proxy
        }
    
    watch_dir = os.path.abspath(args.watch)
    pdf_dir = args.pdf_dir or os.path.join(watch_dir, 'pdfs')
    state_file = os.path.join(watch_dir, '.paper_watcher_state.json')
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(watch_dir, exist_ok=True)
    os.makedirs(pdf_dir, exist_ok=True)
    
    proxy_status = "å·²å¯ç”¨" if CONFIG["use_proxy"] else "å·²ç¦ç”¨"
    proxy_addr = CONFIG.get("proxy", {}).get("http", "N/A") if CONFIG["use_proxy"] else "N/A"
    
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           ğŸ“š è®ºæ–‡ç¬”è®°è‡ªåŠ¨åŒ–å·¥å…· v1.1                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ç›‘æ§ç›®å½•: {watch_dir:<40} â•‘
â•‘  PDFç›®å½•:  {pdf_dir:<40} â•‘
â•‘  ä»£ç†çŠ¶æ€: {proxy_status:<40} â•‘
â•‘  ä»£ç†åœ°å€: {proxy_addr:<40} â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    # åˆå§‹åŒ–ç»„ä»¶
    state = StateManager(state_file)
    processor = MarkdownProcessor(state, pdf_dir)
    
    if args.once:
        # å•æ¬¡æ‰«ææ¨¡å¼
        print("å•æ¬¡æ‰«ææ¨¡å¼...")
        for filename in os.listdir(watch_dir):
            if filename.endswith('.md'):
                filepath = os.path.join(watch_dir, filename)
                print(f"\nå¤„ç†æ–‡ä»¶: {filename}")
                results = processor.process_file(filepath)
                if results:
                    processor.update_file_with_formatted_refs(filepath, results)
        print("\næ‰«æå®Œæˆï¼")
        return
    
    # æŒç»­ç›‘æ§æ¨¡å¼
    handler = PaperWatcherHandler(processor, state)
    observer = Observer()
    observer.schedule(handler, watch_dir, recursive=False)
    observer.start()
    
    print("å¼€å§‹ç›‘æ§... (æŒ‰ Ctrl+C åœæ­¢)")
    print("æç¤º: åœ¨Markdownæ–‡ä»¶ä¸­æ·»åŠ arXivé“¾æ¥ï¼Œä¿å­˜åä¼šè‡ªåŠ¨å¤„ç†\n")
    
    try:
        while True:
            time.sleep(1)
            handler.process_pending()
    except KeyboardInterrupt:
        print("\nåœæ­¢ç›‘æ§...")
        observer.stop()
    
    observer.join()
    print("å·²é€€å‡º")


if __name__ == '__main__':
    main()
